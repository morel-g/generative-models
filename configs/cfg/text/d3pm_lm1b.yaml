# @package _global_
data_type: "lm1b_short"
model_type: "d3pm"

scheme_params:
  nb_time_steps_train: 1000
  nb_time_steps_eval: 1000
  seq_length: 25
  tokenizer_name: "gpt2"
  transition_case: "uniform"
  
training_params:
  epochs: 100
  check_val_every_n_epochs: 3
  batch_size: 64
  batch_size_eval: 64
  lr: 2e-4
  weight_decay: 0.0
  ema_dict:
    use_ema: true
    decay: 0.9999
    use_ema_warmup: true
    power: 0.6666
  gradient_clip_val: 1.0

model_params:
  emb_dim: 512
  nb_heads: 8
  hidden_dim: 1024
  nb_layers: 4
  dropout: 0.1
